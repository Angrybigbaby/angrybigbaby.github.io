---
layout:     post
title:      "关于神经网络和反向传播"
subtitle:   "以及梯度下降"
date:       2025-03-27
author:     "Bigbaby"
tags:
    - 人工智能
    - 神经网络
    - 大模型
    - 深度学习
---

> 反向传播是神经网络训练中用于高效计算梯度的关键算法，通过链式法则从输出层反向逐层计算各参数的梯度，从而优化模型。

### 1. **神经网络基础**
   • **结构**：由输入层、隐藏层、输出层组成，层间通过权重连接。
   • **前向传播**：输入数据经过加权求和和激活函数，逐层传递至输出层，得到预测值。
   • **损失函数**：衡量预测值与真实值的差距（如均方误差、交叉熵）。

### 2. **反向传播的作用**
   • **核心目标**：计算损失函数对每个权重的梯度（导数），指导权重调整以减少误差。
   • **为什么需要**：直接计算每个权重的梯度计算量巨大，反向传播通过链式法则高效求解。

### 3. **链式法则与梯度计算**
   • **链式法则**：复合函数导数为各层导数乘积。例如，损失 \( L \) 对权重 \( w \) 的梯度为：
   ```latex
     [frac{\partial L}{\partial w} = \frac{\partial L}{\partial \text{输出}} \cdot \frac{\partial \text{输出}}{\partial \text{隐藏层}} \cdot \frac{\partial \text{隐藏层}}{\partial w}]
```
   • **反向传播步骤**：
     1. **计算输出层梯度**：从损失函数开始，求导至输出层输入。
     2. **逐层反向传递**：将梯度从后往前传递，计算每一层的权重梯度。
     3. **激活函数导数**：如Sigmoid的导数为 \( \sigma'(x) = \sigma(x)(1-\sigma(x)) \)，ReLU导数为分段函数（输入>0时为1，否则为0）。

### 4. **具体示例（以两层网络为例）**
   • **网络结构**：
     ◦ 输入层 \( x \)，隐藏层 \( h = \sigma(w_1 x) \)，输出层 \( y = \sigma(w_2 h) \)。
     ◦ 损失函数 \( L = \frac{1}{2}(y_{\text{true}} - y)^2 \)。
   • **反向传播过程**：
     1. **输出层梯度**：
        \[
        \frac{\partial L}{\partial y} = y - y_{\text{true}}, \quad \frac{\partial y}{\partial w_2} = y(1-y) \cdot h
        \]
        则 \( \frac{\partial L}{\partial w_2} = (y - y_{\text{true}}) \cdot y(1-y) \cdot h \)
     2. **隐藏层梯度**：
        \[
        \frac{\partial L}{\partial h} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial h} = (y - y_{\text{true}}) \cdot y(1-y) \cdot w_2
        \]
        再求 \( \frac{\partial h}{\partial w_1} = h(1-h) \cdot x \)，最终：
        \[
        \frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial h} \cdot h(1-h) \cdot x
        \]

### 5. **梯度下降与参数更新**
   • **梯度下降公式**：权重更新为 \( w = w - \alpha \cdot \frac{\partial L}{\partial w} \)，其中 \( \alpha \) 为学习率。
   • **批量更新**：通常计算一个批次（batch）数据的平均梯度后统一更新，避免单样本噪声影响。

### 6. **反向传播的流程总结**
   1. **前向传播**：计算预测值及损失。
   2. **反向计算梯度**：从输出层开始，利用链式法则逐层计算各权重梯度。
   3. **更新权重**：使用梯度下降等优化算法调整参数。

### 7. **常见问题**
   • **梯度消失/爆炸**：深层网络中梯度可能过小或过大，可通过调整激活函数（如ReLU）、批标准化等缓解。
   • **计算效率**：反向传播的时间复杂度与正向传播相当，显著优于直接计算每个权重的梯度。

### 图解辅助理解
![反向传播示意图](https://miro.medium.com/v2/resize:fit:1400/1*5rHXZRwXQyVlNRD4ZrkHkw.png)
• **红色箭头**：前向传播路径。
• **蓝色箭头**：反向传播梯度传递路径。

通过反向传播，神经网络能够高效学习复杂模式，成为深度学习的基础。理解其核心在于链式法则的应用及梯度传递的逻辑。
