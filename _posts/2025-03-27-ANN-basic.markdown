---
layout:     post
title:      "关于神经网络和反向传播"
subtitle:   "以及梯度下降"
date:       2025-03-27
author:     "Bigbaby"
tags:
    - 人工智能
    - 神经网络
    - 大模型
    - 深度学习
---

> 反向传播是神经网络训练中用于高效计算梯度的关键算法，通过链式法则从输出层反向逐层计算各参数的梯度，从而优化模型。

### 1. **神经网络基础**

   • **结构**：由输入层、隐藏层、输出层组成，层间通过权重连接。
   
   • **前向传播**：输入数据经过加权求和和激活函数，逐层传递至输出层，得到预测值。
   
   • **损失函数**：衡量预测值与真实值的差距（如均方误差、交叉熵）。

### 2. **反向传播的作用**

   • **核心目标**：计算损失函数对每个权重的梯度（导数），指导权重调整以减少误差。
   
   • **为什么需要**：直接计算每个权重的梯度计算量巨大，反向传播通过链式法则高效求解。

### 3. **链式法则与梯度计算**

   • **链式法则**：复合函数导数为各层导数乘积。例如，损失 \( L \) 对权重 \( w \) 的梯度为：
   
![image](https://github.com/user-attachments/assets/5124eb28-1666-43a5-99a7-693894d0297c)
   
   • **反向传播步骤**：
     
 1. **计算输出层梯度**：从损失函数开始，求导至输出层输入。
     
 2. **逐层反向传递**：将梯度从后往前传递，计算每一层的权重梯度。
     
 3. **激活函数导数**：如Sigmoid的导数为![image](https://github.com/user-attachments/assets/ae567ab9-a453-43cc-ac02-40ea02dd054a)，ReLU导数为分段函数（输入>0时为1，否则为0）。

### 4. **具体示例（以两层网络为例）**

   • **网络结构**：
   
◦ 输入层 ![image](https://github.com/user-attachments/assets/4909eca0-2fb2-4d69-84ec-3f881d9647d8)

◦ 损失函数 ![image](https://github.com/user-attachments/assets/48d3ed78-dc3c-4614-b725-f15e39d9c4eb)

   • **反向传播过程**：
   
◦  **输出层梯度**：
![image](https://github.com/user-attachments/assets/0858894d-95e8-4dc0-9344-99258c2b184b)

◦ **隐藏层梯度**：
![image](https://github.com/user-attachments/assets/a9bfa1db-4923-4c5a-a46c-c0aae318447d)

### 5. **梯度下降与参数更新**

• **梯度下降公式**：
   
   权重更新为 ![image](https://github.com/user-attachments/assets/d37f303d-3869-431f-9f34-d650cba4e9ca)，其中 ![image](https://github.com/user-attachments/assets/c009ef4c-0e97-460e-8426-ff9468e59c2a)为学习率。
   
• **批量更新**：
   
   通常计算一个批次（batch）数据的平均梯度后统一更新，避免单样本噪声影响。

### 6. **反向传播的流程总结**

   1. **前向传播**：计算预测值及损失。
   
   2. **反向计算梯度**：从输出层开始，利用链式法则逐层计算各权重梯度。
   
   3. **更新权重**：使用梯度下降等优化算法调整参数。

### 7. **常见问题**

   • **梯度消失/爆炸**：深层网络中梯度可能过小或过大，可通过调整激活函数（如ReLU）、批标准化等缓解。
   
   • **计算效率**：反向传播的时间复杂度与正向传播相当，显著优于直接计算每个权重的梯度。

通过反向传播，神经网络能够高效学习复杂模式，成为深度学习的基础。理解其核心在于链式法则的应用及梯度传递的逻辑。
