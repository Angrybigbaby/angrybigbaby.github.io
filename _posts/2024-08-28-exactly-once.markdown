---
layout:     post
title:      "关于端到端精准一致性语义的具体实践"
subtitle:   "在不过分牺牲吞吐量的情况下实现极致的准确性"
date:       2024-08-28
author:     "Bigbaby"
tags:
    - Kafka
    - Flink
    - Hudi
---

> 数据从源端产生到入仓，中间由于涉及不同的网络环境以及不同的网安等级，因此在架构上分别定义为为数据引接阶段和数据采集阶段。我们在引接区部署Java代码进程来采集数据，并生产到中转Kafka。采集区FlinkJob消费中转Kafka，将数据入湖或ETL。
>
> 因此我们从引接区和采集区两个方面来阐述端到端精准一致性语义的具体实现。

## 引接区精准一致性语义

引接区的一致性语义实现主要体现在生产入中转Kafka环节的一致性保证。对于源端的精准一致性，作为下游开发来讲我们是无能为力的。所以我们能做的工作就体现在对Kafka的优化上。作为Kafka生产者，以下分别阐述生产不丢失和生产不重复的实现思路。

Kafka的Ack + Retry机制是保证生产不丢失的基本因素（at least once）。

Ack应答是Kafka本身具备的机制，生产者生产数据写入Kafka，等待Kafka返回Ack确认。收到Ack，生产者发送下一条。

对于Ack机制的应用主要体现在生产者的配置上，有0，1，all（-1）三种参数可以配置。

其中0代表不采用Ack，生产者发送一条数据后不等待Ack即发送下一条。这种方式不易形成数据积压，但是发生数据丢失的风险较大，尤其是大数据量场景。

1代表由分区Leader副本发回Ack，生产者接收到Ack开始发送第二条数据，同时Follower副本向Leader副本同步数据。这种方式对数据丢失的风险和生产数据的效率做了平衡，是目前企业生产环境最广泛采用的配置。

而All（-1）是Leader副本接收到数据，且等待Follower副本完成数据同步后才发出Ack。这种配置数据安全性最高，但是生产效率也最低下。目前生产环境采用All的配置，后续是否需要改变有待评估。

通过设置Retry参数可以设定一条数据发送失败的重试次数。当一条数据没有接收到Ack即发送失败，通过Retry机制再次重试发送，这种方式可以确保该条数据写入Kafka，从而保证了生产不丢失。

Kafka的幂等性机制可以保证生产不重复（At Most Once）。所谓幂等性是指一个操作被执行多次，结果是一致的。Kafka通过幂等性机制在数据中增加数据ID，每条数据的数据ID都不一致。

添加唯⼀ID，类似于数据库的主键，⽤于唯⼀标记⼀个消息。Kafka为了实现幂等性，它在底层设计架构中引⼊了ProducerID和SequenceNumber。在每个新的Producer初始化时，会被分配⼀个唯⼀的ProducerID，这个ProducerID对客户端使用者是不可见的。

对于每个ProducerID，Producer发送数据的每个Topic和Partition都对应⼀个从0开始单调递增的SequenceNumber值。Kafka会判断每次要写入的id是否比上一次的id多1，如果多1，就写入，不多1，就直接返回Ack。

该机制保证了一条数据如果与上一条重复就无法写入，理论上杜绝了发生重复生产的可能性。

但在实际情况中，Ack=All**并不能够完全保证生产不丢失**。原因是对于一个Kafka生产者而言，有同步发送与异步发送两种方式。

对于异步发送能够最大化生产者吞吐量，然而由于没有明确接收到上一条（或批）数据的Ack就执行下一条（或批）数据的发送操作，可能导致前面一条数据的丢失。

在同步发送的模式中，生产者客户端会在收到Kafka的Ack告知发送成功之前一直处于阻塞状态，只有收到上一条数据的Ack后才会执行下一条Send（发送）操作。

这种方式牺牲了吞吐量，同时保证了可靠性。因此对于Kafka生产者，同步发送配合Ack才能够提供强力的一致性语义保证。

然而考虑到同步发送对吞吐量的负面影响<s>（负面影响极大）</s>，在项目中为了平衡性能和一致性，采用异步发送配合重试机制。<s>没办法，为了性能做出的妥协</s>

相比于同步发送，异步发送具备吞吐量优势，且可以在异步发送的基础上采用批量发送。

由于不需等待每条数据的Ack，可以通过指定batch.size和linger.ms来实现每次发送多条数据，最大化生产者性能。如果某条数据发送失败，那么会根据设定的Retry次数来尝试重发。

这种方式的弊端在于，如果Retry过程中发送成功将导致数据顺序被打乱。

在异步发送的大前提上，采用带有回调函数的异步发送。回调函数的作用是接收Ack，并在重写onCompletion方法中接收异常信息，并发送相应的监控告警信息。

带有回调函数的异步发送提高了异步发送的一致性保证，综合性能和一致性两方面因素考虑，为最佳解决方案。

除同步发送外，引入生产者事务可以提供更强的生产者一致性的理论保证。其事务机制的具体实现为，在调用Send方法预提交后，通过commitTransaction正式提交一个事务。

由于考虑Kafka可能扩展为多节点多分区的架构，那么幂等性机制就存在局限性，因为幂等性无法保证多分区下的生产不重复。而引入事务性生产者可以提供多分区场景下的一致性语义保证，因为事务的ACID特性，Kafka 可以保证跨多个Topic-Partition的数据要么全部写入成功，要么全部失败，不会出现中间状态。

这种两阶段提交的方式可以实现生产者的一致性语义。

## 采集区精准一致性语义

在Kafka + Flink + Hudi架构中，端到端的精准一致性语义由两个外部系统（Kafka + Hudi）及Flink内部实现。以下分别阐述Flink内部及外部的一致性语义实现。

Flink内部的一致性语义主要由Checkpoint机制来保证。Checkpoint采用分布式快照算法实现一致性，是Flink实现容错机制最核心的功能。

它能够根据配置周期性地基于Stream中各个Operator/Task的状态State来生成快照，从而将这些状态数据定期持久化存储下来，当Flink程序一旦意外崩溃时，重新运行程序时可以有选择地从这些快照进行恢复，从而修正因为故障带来的程序数据异常。

Flink状态分为键控状态与算子状态，本质是某Task/Operator在某时刻的一个中间结果。

由于流计算大部分场景为增量计算，因此在发生故障后，能实现基于上一时刻的中间结果继续工作，即为Flink实现**精准一致性语义的本质**。

自Flink1.13以后，单独划分了Checkpoint Storage的概念。Checkpoint的存储分为两种方式：JobManager内存或FileSystem文件系统。

考虑到内存的不稳定性，企业一般采用FileSystem（本地路径或分布式文件系统）的方式持久化存储Checkpoint。在本项目中，采用HDFS作为Checkpoint的存储介质，这种方式保证了Checkpoint的安全性。这样一来，Flink内部的精准一致性语义也就得以实现。

在Source端的一致性语义实现的本质是Kafka的Offset持久化存储。在开启Flink Checkpoint后，Kafka Offset存储在Flink State中。

Flink流环境中有setCommitOffsetsOnCheckpoints配置，该配置默认为true，即由Ckeckpoint触发时提交Offset。

当一个FlinkJob在完成第一次Checkpoint后失败，那么在完成第一次Checkpoint后消费的Kafka数据对应的Offset不会提交。

当FlinkJob重启后，会在第一次Checkpoint的时间节点继续运行，所消费的数据也是第一次Checkpoint时对应的Offset + 1，因此不会发生数据重复或丢失。

HDFS存储的方式保证了Checkpoint的安全性，也就意味着保证了Source端的一致性语义实现。

在Sink端的一致性语义的实现仍然依赖Checkpoint。Hudi每次Commit会将一批数据原子性地写入一个表，其对应Timeline的一个新Instant。

而当Commit/Deltacommit不成功时Hudi会进行回滚，其会删除在写入过程中产生的部分文件。这种近似事务回滚的特性使Hudi能够保证数据不丢失不重复。

由于Hudi的Commits操作依赖于Flink Checkpoint，因此Commits的安全性也得以保证，并与Checkpoint机制实现统一。

> 以上
